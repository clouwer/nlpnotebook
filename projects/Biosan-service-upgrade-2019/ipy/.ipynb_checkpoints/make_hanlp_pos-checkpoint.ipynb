{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "sys.path.extend(['../src/'])\n",
    "from config import *\n",
    "config = Config()\n",
    "\n",
    "def data_sql_load():\n",
    "    engine = create_engine('mysql+pymysql://%s:%s@%s:3306/%s?charset=utf8' %(config.sql_user, config.sql_password, config.sql_ip, config.sql_database),echo = False)\n",
    "    sql = \"select * from etl_data\"\n",
    "    data = pd.read_sql_query(sql, engine)\n",
    "    return data\n",
    "\n",
    "data = data_sql_load()\n",
    "\n",
    "data['text'] = data['问题汇总'] + data['解决方案汇总']\n",
    "\n",
    "from pyhanlp import *\n",
    "\n",
    "def hanlp_adduserdict(input_file, nature):\n",
    "    with open(input_file,  encoding='UTF-8') as f:\n",
    "        lines = []\n",
    "        words = []\n",
    "        for line in f:\n",
    "            contends = line.strip()\n",
    "            word = line.strip().split(' ')[0]\n",
    "            if contends.startswith(\"-DOCSTART-\"):\n",
    "                words.append('')\n",
    "                continue\n",
    "            # if len(contends) == 0 and words[-1] == '。':\n",
    "            if len(contends) == 0:\n",
    "                w = ' '.join([word for word in words if len(word) > 0])\n",
    "                lines.append([w])\n",
    "                words = []\n",
    "                continue\n",
    "            words.append(word)\n",
    "    CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "    for w in words:\n",
    "        CustomDictionary.insert(w, nature)\n",
    "        \n",
    "label_dict = {\n",
    "    'bd': 'BODY',\n",
    "    'err': 'ERRORS'\n",
    "}\n",
    "\n",
    "config.ner_train_data = '../source/words/ner_train.txt'\n",
    "\n",
    "hanlp_adduserdict('../source/words/bodys.txt', 'bd 1024')\n",
    "hanlp_adduserdict('../source/words/errors.txt', 'err 1024')\n",
    "Segment = HanLP.newSegment()\n",
    "\n",
    "def hanlp_pos(text):\n",
    "    f = open(config.ner_train_data, 'a', encoding = 'UTF-8')\n",
    "    sg = Segment.seg(text)\n",
    "    sg_word = [str(i.word) for i in sg]\n",
    "    sg_nature = [str(i.nature) for i in sg]\n",
    "\n",
    "    words = [i for i in sg_word if sg_nature[sg_word.index(i)] in ['bd', 'err']]\n",
    "    words_begin = [text.index(i) for i in sg_word if sg_nature[sg_word.index(i)] in ['bd', 'err']]\n",
    "    words_end = [text.index(i)+ len(i)-1 for i in sg_word if sg_nature[sg_word.index(i)] in ['bd', 'err']]\n",
    "    words_nature = [i for i in sg_nature if sg_word[sg_nature.index(i)] in words]\n",
    "\n",
    "    content = text.strip()\n",
    "    res_dict = {}\n",
    "    for w in range(len(words)):\n",
    "        res = words[w]\n",
    "        start = words_begin[w]\n",
    "        end = words_end[w]\n",
    "        label = label_dict[words_nature[w]]\n",
    "        for i in range(start, end+1):\n",
    "            if i == start:\n",
    "                label_cate = 'B-' + label\n",
    "            else:\n",
    "                label_cate = 'I-' + label\n",
    "            res_dict[i] = label_cate\n",
    "\n",
    "    for indx, char in enumerate(content):\n",
    "        char_label = res_dict.get(indx, 'O')\n",
    "        f.write(char + '\\t' + char_label + '\\n')\n",
    "    f.close()\n",
    "\n",
    "from tqdm import tnrange\n",
    "        \n",
    "for i in tnrange(data.shape[0]):\n",
    "    try:\n",
    "        hanlp_pos(data['text'][i])\n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
