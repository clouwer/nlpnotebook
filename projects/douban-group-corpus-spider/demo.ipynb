{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from config import GROUP_DICT, MAX_PAGE, SQL_DICT, HEADERS, PROXY_POOL_URL, MAX_GET_RETRY, OUTPUT_PATH,SPIDER_INTERVAL\n",
    "from base import _Sql_Base\n",
    "import requests\n",
    "import emoji\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "\n",
    "class HTTPError(Exception):\n",
    "\n",
    "    \"\"\" HTTP状态码不是200异常 \"\"\"\n",
    "\n",
    "    def __init__(self, status_code, url):\n",
    "        self.status_code = status_code\n",
    "        self.url = url\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s HTTP %s\" % (self.url, self.status_code)\n",
    "\n",
    "def get_logger(name):\n",
    "    \"\"\"logger\n",
    "    \"\"\"\n",
    "    default_logger = logging.getLogger(name)\n",
    "    default_logger.setLevel(logging.DEBUG)\n",
    "    stream = logging.StreamHandler()\n",
    "    stream.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\")\n",
    "    stream.setFormatter(formatter)\n",
    "    default_logger.addHandler(stream)\n",
    "    return default_logger    \n",
    "\n",
    "class Douban_corpus_spider(_Sql_Base):\n",
    "\n",
    "    def __init__(self, is_proxy = False):\n",
    "\n",
    "        self.GROUP_DICT = GROUP_DICT\n",
    "        self.MAX_PAGE = MAX_PAGE\n",
    "        self.sql_engine = self.create_engine(SQL_DICT)\n",
    "        self.is_proxy = is_proxy\n",
    "        if is_proxy:\n",
    "            self.proxyIP = self.get_proxy()\n",
    "        self.logger = get_logger(\"douban_spider\")\n",
    "            \n",
    "    def request_douban(self, url):\n",
    "        headers = {\n",
    "            'User-Agent': HEADERS['GalaxyS5']\n",
    "        }\n",
    "        for i in range(MAX_GET_RETRY):\n",
    "            try:\n",
    "                if self.is_proxy:\n",
    "                    proxyIP = self.proxyIP\n",
    "                    proxies = {\n",
    "                        'http' : proxyIP,\n",
    "                        'https': proxyIP\n",
    "                    }\n",
    "                    response = requests.get(url, proxies=proxies, headers=headers)\n",
    "                else:\n",
    "                    response = requests.get(url, headers=headers)\n",
    "                if response.status_code != 200:\n",
    "                    raise HTTPError(response.status_code, url)\n",
    "                print('successfully get data from %s' %url)\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                self.logger.warn(\"%s %d failed!\\n%s\", url, i, str(exc))\n",
    "                if self.is_proxy:\n",
    "                    self.proxyIP = self.get_proxy()\n",
    "                continue\n",
    "        time.sleep(2)\n",
    "        return response.text\n",
    "    \n",
    "    # 从代理池中随机取出一个IP\n",
    "    def get_proxy(self):\n",
    "        try:\n",
    "            response = requests.get(PROXY_POOL_URL)\n",
    "            if response.status_code == 200:\n",
    "                print('proxy: %s' %response.text)\n",
    "                time.sleep(1)\n",
    "                return \"http://%s\" %response.text\n",
    "        except ConnectionError:\n",
    "            return None\n",
    "\n",
    "    def spider_links(self, group, page):\n",
    "        url = '{}discussion?start={}'.format(self.GROUP_DICT[group], str(page*25))\n",
    "        html = self.request_douban(url)\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        list_ = soup.find(class_='olt').find_all('tr')\n",
    "        page_link = []; page_title = []\n",
    "        for item in list_:\n",
    "            try:\n",
    "                page_link.append(item.find('a').get('href'))\n",
    "                page_title.append(item.find('a').get('title'))\n",
    "            except:\n",
    "                continue\n",
    "        return page_link, page_title\n",
    "\n",
    "    def spider_page(self, url):\n",
    "        html = self.request_douban(url)\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        page_author_diag = []\n",
    "        for item in soup.find(class_=\"note-content paper\").find_all('p'):\n",
    "            if len(item) > 0:\n",
    "                page_author_diag.append(item.contents[0])\n",
    "        list_ = soup.find_all(class_ = 'content')\n",
    "        page_comments = []\n",
    "        for item in list_:\n",
    "            page_comments.append(item.contents[2].replace(' ','').replace('\\n',''))\n",
    "        return page_author_diag, page_comments\n",
    "\n",
    "    def spider_group(self, group, max_page):\n",
    "        spider_outputs = {}\n",
    "        link_list = []\n",
    "        title_list = []\n",
    "        for page in range(max_page):\n",
    "            link_list_page, title_list_page = self.spider_links(group, page)            \n",
    "            link_list = link_list + link_list_page\n",
    "            title_list = title_list + title_list_page\n",
    "        for link in link_list:\n",
    "            try:\n",
    "                spider_outputs[link] = {}\n",
    "                spider_outputs[link]['title'] = title_list[link_list.index(link)]\n",
    "                spider_outputs[link]['author_diag'], spider_outputs[link]['comments'] = self.spider_page(link)\n",
    "                self.json_write(spider_outputs, os.path.join(OUTPUT_PATH, '{}.json'.format(group)))\n",
    "            except:\n",
    "                print('%s fail' %link)\n",
    "                continue        \n",
    "        return spider_outputs\n",
    "\n",
    "    def group_dict_transfer(self, output_dict):\n",
    "        data = pd.DataFrame(output_dict).T\n",
    "        data['link'] = data.index\n",
    "        data = data.reset_index(drop = True)[['link','title','author_diag','comments']]\n",
    "        data['author_diag'] = data['author_diag'].apply(lambda x: x[0] if x else '')\n",
    "        data['comments'] = data['comments'].apply(lambda x: '|'.join(x))\n",
    "        for col in ['title','author_diag','comments']:\n",
    "            data[col] = data[col].apply(emoji.demojize)\n",
    "        return data\n",
    "\n",
    "    def run(self):\n",
    "        for group in self.GROUP_DICT.keys():\n",
    "            output_dict = self.spider_group(group, self.MAX_PAGE)\n",
    "            output_table = self.group_dict_transfer(output_dict)\n",
    "            self.table_save(output_table, group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
